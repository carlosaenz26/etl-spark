{"cells":[{"cell_type":"markdown","source":["> Se importan las librerías a utilizar"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c1d5f44-69a6-45ad-a8c9-f7da0d77a4d5"}}},{"cell_type":"code","source":["import numpy as np\n\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.functions import col\n\nfrom pyspark.sql.types import StringType,DoubleType\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import *\n\nspark =SparkSession.builder.appName(\"mi_app\").getOrCreate()\nfrom decimal import *\nsc=spark.sparkContext\n\nfrom pyspark.sql.functions import lit\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StringIndexer,OneHotEncoder, VectorAssembler\nfrom pyspark.sql.functions import col\nimport csv\nimport numpy as np\nimport pandas as pd\nimport gzip\nfrom datetime import date\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom numpy import arange\nfrom matplotlib import pyplot\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom pyspark.ml.feature import OneHotEncoder\nfrom pyspark.sql.functions import col\n\nfrom pyspark.sql.types import StringType,DoubleType\nfrom pyspark.sql.types import (StringType, BooleanType, IntegerType, FloatType, DateType)\n  \nfrom pyspark.sql.functions import col,isnan, when, count\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import row_number\nfrom pyspark.sql import Window\n\n\n#Librerias de Spark que se utilizan en la generacion del modelo de regresion lineal \nfrom pyspark.sql.functions import corr\n\n\nfrom pyspark.ml.regression import LinearRegression\n\n\n\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fec347a0-29f4-484b-bb9d-176187086d67"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh\npip install mlflow\n/databricks/python_shell/scripts/PythonShell.py configure"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0ff89a2-e8f1-4fc5-9f9d-26049c29f433"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["> Se importan las 3 bases que fueron el resultado del proceso de preparación de los datos:\n\n> Se hace un filtrado para determinadas columnas\n\n> Se categorizan valores numéricos"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c8d0e29d-ea32-47d4-85bf-f3bd969bd7b6"}}},{"cell_type":"code","source":["#raw_datax = spark.read.options(inferSchema='True',header='True')('/dbfs/gold/icbf-gai/BaseModeloProbabilisticoSabana')\n\n\n\npath_to_data=\"dbfs:/dbfs/gold\"\nraw_datax = spark.read.options(inferSchema='True',header='True').format(\"delta\").load(path_to_data)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"69368d2e-dd8c-460b-8412-d7a18f3ebfd3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndef tiposendf(raw_datax):# funcion que da los tipos en el df en una lista \n    tipos=raw_datax.dtypes\n    ele=[]\n    for i in range(len(tipos)):\n        ele.append(tipos[i][1])\n    uniq = []\n    for x in ele:\n        if x not in tipo:\n            uniq.append(x)\n            tipo.add(x)\n    return(uniq)\n  \ndef filtrotype(df, tipo):# funcion que da las columnas de cierto tipo en especifico \n    my_list = []\n    for i in df.dtypes:\n        if i[1].startswith(tipo):\n            my_list.append(i)\n    return my_list\n\ndef filtro(df, ini,fin): # filtro al cual entra como parametro un df y 2 palabras claves de inicio y fin del nombre de las columnas y devuelve un df que no tuene dichas columnas \n    lista=df.columns\n    my_list = []\n    for i in lista:\n        if i.startswith(ini) and i.endswith(fin):\n             my_list.append(i)\n        \n    df = df.drop(*my_list)\n    return df\n\ndef filtro1(df, key,idi): # funcion  a la cual entra un df y dos palabras clves entre comillas y devuelve la lista de las columnas que tienen esas palabras \n    lista=df.columns\n    arr = np.array(lista)\n    my_list = []\n    for i in arr:\n        #for j in len(lista[i]):\n        if key in i:\n            my_list.append(i)\n                \n        if idi in i:\n            my_list.append(i)\n    return my_list\n\n ## fun repetidas \n\ndef filtro2(df, key): # filtro al cual entra como parametro un df y  una palabra clave en comillas y devuelve una lista con las columnas del df que contienen esa palabra clave \n    lista=df.columns\n    arr = np.array(lista)\n    my_list = []\n    for i in arr:\n        #for j in len(lista[i]):\n        if key in i:\n            my_list.append(i)\n                \n    return my_list\n\ndef filtrofin(df, fin): # Filtro al cual entra un df y una palabra clave y  devuelve una lista con el nombre de todas las columnas que terminen con dichas ´palabras clave \n    lista=df.columns\n    my_list = []\n    for i in lista:\n        if  i.endswith(fin):\n             my_list.append(i)\n\n    return my_list"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0422c5b0-1f88-48a1-ac9f-589dc627ce7f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["len(raw_datax.columns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad11299a-ad43-4548-b1d7-8f33adc99e30"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# limpieza \n\n## se utilizan los filtros para eliminar columnas repetidas o que no aportan informacion relevante, como identificacion por nombres, direcciones, fechas o columnas que ya estan modeladas por otras columnas en el df \n\nse eliminan 210 columnas con dichos filtros"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f996a841-2f89-4733-aa2b-becf9a9028a1"}}},{"cell_type":"code","source":["nom=filtro1(raw_datax, 'nom','Nom')\nidi=filtro2(raw_datax, 'IdBeneficiarioT')\nins =filtro2(raw_datax, 'INS')\nDir=filtro1(raw_datax, 'Dir','dir')\nfecha=filtro1(raw_datax, 'Fecha','fecha')\nflag=filtro1(raw_datax, 'Flag','PresentaCarneVacunacion')\nNoTomaT=filtro1(raw_datax, 'Flag','NoTomaT')\nTomaT=filtro2(raw_datax, 'TomaT')\nraw_datax=raw_datax.drop(*nom)\nraw_datax=raw_datax.drop(*idi)\nraw_datax=raw_datax.drop(*Dir)\nraw_datax=raw_datax.drop(*fecha)\nraw_datax=raw_datax.drop(*flag)\nraw_datax=raw_datax.drop(*NoTomaT)\nraw_datax=raw_datax.drop(*TomaT)\nraw_datax=raw_datax.drop(*ins)\n\nlisteli=['depmuni','depto','munic','zona','sector','seccion','manzana','comuna','barrio','vereda','NOM_DPTO','NOM_MPIO','NumeroDocumento',\n'SegundoApellido','PrimerApellido','NumeroDocumento','NoTomaT','ficha','teles','ape1','ape2','documen','ind_estudia','tip_origen_agua','ind_tiene_alcantarillado','tip_mat_paredes','tip_mat_pisos','ind_tiene_acueducto','num_cuartos_dormir','tip_sanitario','ide_ficha_origen','num_ficha', 'Jefe_UG',  'seg_nom_informante',\n'tip_cuidado_niños',\n'ide_firma_informante',\n'Email_contacto',\n'Cod_dpto',\n'Cod_mpio',\n'coord_x_manual_rec',\n'coord_y_manual_rec',\n'coord_x_auto_rec',\n'coord_y_auto_rec',\n'Dir_Chip',\n'ide_persona',\n'ind_escaner',\n'pri_apellido',\n'seg_apellido',\n'pri_nombre',\n'seg_nombre',\n'sexo_persona',\n'tip_documento',\n'num_documento',\n'fec_nacimiento',\n'edad_calculada',\n'fec_documento',\n'cod_mpio_documento',\n'cod_dpto_documento',\n'cod_pais_documento',\n'ind_discap_ver',\n'ind_discap_oir',\n'ind_discap_hablar',\n'ind_discap_moverse',\n'ind_discap_bañarse',\n'ind_discap_salir',\n'ind_discap_entender',\n'ind_discap_ninguna',\n'ind_fondo_pensiones',\n'tip_actividad_mes',\n'num_sem_buscando',\n'tip_empleado',\n'Ide_hogar',\n'ide_informante',\n'pri_nom_informante',\n'seg_nom_informante',\n'pri_ape_informante',\n'seg_nom_informante',\n'ide_firma_informante',\n'Email_contacto',\n'num_tel_contacto',\n'Grupo',\n'Nivel',\n'fec_actualizacion_cns',\n'tip_mat_paredes',\n'tip_mat_pisos']\nraw_datax=raw_datax.drop(*listeli)\n#ZonaUbicacionBeneficiario"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"073db8d7-2eba-41e3-a0eb-7c5baa795fa4"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# funciones para limpiar y crear las sabanas de datos"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ba936f8-8775-4728-b0b6-8236331f2565"}}},{"cell_type":"code","source":["def nulls(df):# funcion que da el procentaje de nulls por columna de un df de entrada\n    cols=df.columns\n    rows=df.count()\n    df=df.withColumn('numNulls1',sum(df[col].isNull().cast('int') for col in cols))\n    Dict_Null = {col:df.filter(df[col].isNull()).count()/rows for col in cols}# crea un diccionario con\n    #el nombre de la columna y la cantidad de nulls en    dicha columna\n    val=Dict_Null.values() \n    key=Dict_Null.keys() \n    key1=list(key)#lista con nombre de la columna\n    filt=[]\n    val1=list(val)\n    return(Dict_Null)\n    # crea un diccionariocon el nombre de la columna y la cantidad de nulls en dicha columna\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a7d05e3-98d6-41f5-b0a2-389731458bc7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# la funcion filtro 4 elimina las columnas asociadas a las tomas no deseadas se debe especificar el # de tomas que se quieren tener en cuenta, \n#luego elimina las columnas que tienen un porcentaje superior de nulls al deseado el cual entra por parametro \n#devuleve un df que segun un porcentaje de nulls predefinido acota la cantidad de columnas \ndef filtro4(df,Notomas,porcentaje):\n    df=df.select([when(col(c)==\"\",\"Null\").otherwise(col(c)).alias(c) for c in df.columns]) #llena las columnas con null \n    p=df.select(filtro1(df, 'EstadoPesoTalla', 'IdBeneficiario')) # selecciona solo las columnas EstadoPesoTalla\n    p1=p.drop(*filtro2(df, 'IdBeneficiarioT')) #dataframe sin las columnas  IdBeneficiarioT\n    listaEPT=p1.columns # lista con las columnas EPT+ Idbeneficiario\n    p1=p1.withColumn('numNulls', sum(p1[col].isNull().cast('int') for col in listaEPT)) #crea la columna numNulls que indica la cantidad de nulls en las  columnas EstadoPesoTalla. p1 un data frame con las columnas estado peso el id y la columna numnulls\n    f=len(listaEPT) # cantidad de tomas mas el id del ben sin um nulls\n    #h=(18-Notomas)\n    h=(f-Notomas+2)# esto!\n    p1 =p1.filter(p1.numNulls== h) # filtra las columnas segun el no de tomas deseado\n    listaEPTf=p1.columns # cantidad de tomas mas el id del ben sin columna num nulls\n    \n    lista3=listaEPTf[Notomas:]# lista con las columnas no deseadas \n    \n    p1=p1.drop(*lista3) # dataframe con las columnas de las EPT deseadas\n    colfin=p1.columns\n    \n    lista4=p1.select('IdBeneficiario').rdd.map(lambda x : x[0]).collect()# hace una lista con los usuarios que cumplen la condicion de tener las tomas \n    #parte2\n    \n    df1=df.filter(df.IdBeneficiario.isin(lista4)) # elimina las filas de los usuarios que no cumplen en el df de entrada\n    #lista5=df1.columns #columnas finale \n    #df1=df1.withColumn('numNulls1', sum(df1[col].isNull().cast('int') for col in lista5)) # crea una columna contando los nulls por fila \n    df1=df1.drop(*lista3)\n    r=df1.count()\n    c=df1.columns\n    Dict_Null = {col:df1.filter(df1[col].isNull()).count() for col in c}# crea un diccionariocon el nombre de la columna y la cantidad de nulls en dicha columna\n    val=Dict_Null.values() \n    key=Dict_Null.keys() \n    key1=list(key)#lista con nombre de la columna\n    filt=[]\n    val1=list(val) # lista con la cantidad de nulls\n    for i in range(len(val1)):# calculo del porcentaje de nulls en dicha columna comparado con el numero total de filas \n        if val1[i]/r > porcentaje:# mayor al porcentaje se elimina columna\n            filt.append(key1[i])# lista con las columnas que tienen un porcentaje de nulls mayor al entrado por parametro\n    #print(filt)\n    df2=df1.drop(*filt)# elimina dichas columnas \n    #hasta aca esta bien\n    \n    notomas=(range(1,Notomas+2,1))\n    lista6=[]\n    for c in notomas: # crea una lista con el nombre de las columnas de las tomas que no se consideran \n        name='T'\n        if c in lista3:\n            #lista6.append(filtrofin(df2, \"%s%s\"%(name,c))\n            lista6.extend(filtrofin(df2, \"%s%s\"%(name,c) ))\n    df2=df2.drop(*lista6) # elimina las columnas relacionadas con las tomas que no se tienen en cuenta \n    return df2"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5be26381-9388-4db1-bc73-0a0732de34d3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndef isInt(num):\n    try:\n        int(num)\n        return True\n    except ValueError or TypeError:\n        return False\n    \ndef isfloat(num):\n    try:\n        float(num)\n        return True\n    except ValueError or TypeError:\n        return False\n\ndef catInt(df): # esta funcion  evalua todas las columnas del df y determina si es int u la guarda en una lsta \n    f=df.first()\n    c=df.columns\n    inte=[]\n    for i in range(0,len(f),1):\n        if isinstance(f[i], int)==True and type(f[i])== int:\n            inte.append(c[i])\n    return(inte)\n\ndef catFlo(df): # esta funcion  evalua todas las columnas del df y determina si es float u la guarda en una lsta \n    f=df.first()\n    c=df.columns\n    #fp=f.topandas()\n    flo=[]\n    inte=catInt(df)\n    for i in range(0,len(f),1):\n        if isinstance(f[i], float)==True and type(f[i])== decimal.Decimal :\n            flo.append(c[i])\n            \n    return(flo)\ndef catstring(df): # esta funcion  evalua todas las columnas del df y determina si es String u la guarda en una lsta \n    f=df.first()\n    c=df.columns\n    st=[]\n    for i in range(0,len(f),1):\n        if isinstance(f[i], str)==True and type(f[i])!=int and type(f[i])!=type(Decimal(3.14)) and isInt(f[i])!=True:\n            st.append(c[i])\n    return(st)\n\ndef categorical(df):\n    c=df.columns\n    catego=[]\n    \n    for i in range(0,len(c),1):\n        if c[i].unique()<50:\n              catego.append(c(i))\n    return(catego)     \n\nfrom pyspark.sql.functions import col\n# esto funciona bien \n\ndef castype(df): # esta funcion determina la categoria de cada una de las columnas \n    f=df.first() #primera fila del df\n    c=df.columns \n    inte=catInt(df)\n    flo=catFlo(df)\n    st=catstring(df)\n    for i in st:\n        df.select(col(\"%s\"%(i)).cast('string').alias(\"%s\"%(i)))\n    for j in inte:\n        df.select(col(\"%s\"%(j)).cast('int').alias(\"%s\"%(j)))\n    for k in flo:\n        df.select(col(\"%s\"%(k)).cast('float').alias(\"%s\"%(k)))\n    return(df)\n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9d02765-4854-4513-a950-157fe25fc516"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#creacion variable objetivo\n\n## se binariza el historial de los estados peso talla y se elimina la forma categorica de estos \n## Segun el comportamiento de las columnas estado peso talla se crea la columna objetivo que representa el historial de reincidencia"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e46cbc39-25d9-4981-b203-687505883695"}}},{"cell_type":"code","source":["def ept(df):# devuelve el df con las columnas ept mapeados con los codigos\n    p=df.select(filtro1(df, 'EstadoPesoTalla', 'IdBeneficiario')) # selecciona solo las columnas EstadoPesoTalla\n    eptt=filtro2(df, 'EstadoPesoTalla') # lista de las columnas \"estado peso talla\" \n    p1=p.drop(*filtro2(p, 'IdBeneficiarioT')) #elimina IdBeneficiarioT\n    listaEPT=p1.columns\n    soloept=filtro2(p, 'EstadoPesoTalla')\n    listaEPT=len(soloept)\n    \n    def somefunc(value):\n        if value== 'Peso adecuado para la talla': \n                  return int(0)\n        elif value=='Riesgo de desnutrición aguda':\n            return int(0)\n        elif value=='Desnutricion Aguda Moderada':\n            return int(1)\n        elif value=='Riesgo de Sobrepeso':\n            return int(0)\n        elif value=='Desnutricion aguda severa':\n            return int(1)\n        elif value=='Obesidad':\n            return int(0)\n        elif value=='Sobrepeso':\n            return int(0)\n        else:\n            return int(0)\n    udfsomefunc = F.udf(somefunc, IntegerType())\n    EPTs=[]\n    for c in (range(1,listaEPT+1)):\n         EPTs.append('EPT%s'%(c))\n    for i in range(listaEPT):\n        df= df.withColumn(EPTs[i], udfsomefunc(df[soloept[i]]))\n        #df= df.drop(*eptt)\n    return   df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bd0b8f2-b8c8-4601-980e-21d033d549cc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def reincidenciaff2(df):#\n    soloept=filtro2(df,'EPT')# se crea una lista con las columnas estado peso talla binario\n    c=df.select(soloept) #se crea el data frame con dichas columnas \n    listu = c.collect() #se obtienen todas las filas de dicho df \n    s=len(listu[1])# la cantidad de columnas \n    x=range(s,0,-1)\n    l=len(listu) #la dimension de la matriz \n    t=range(0,l,1)\n    ant=0\n    rein=[]\n    for j in t :\n        r1 = False # formatea el estado de la bandera para cada fila\n        for i in x: # se recorre las filas \n\n            if i==s:# primera iteracion de la ultima a la primera\n\n                ant=listu[j][i-1] # primera iteracion ant = al valor ultima toma\n\n            else:\n                #print(0)\n                if ant!=listu[j][i-1]:# si el anterior es diferente al actual y si r1 = true : r2=true y hay incidencia \n                    #print(ant)\n\n                    if r1== True :\n                        rein.append(1)\n                        break  \n                    ant=listu[j][i-1] # se actualiza el anterior\n                    r1=True\n            if s-i==s-1:# si el ultimo elemento es igual al penultimo elemento\n                rein.append(0)\n                break\n            \n    \n    \n    Ids=df.select('IdBeneficiario').rdd.flatMap(lambda x: x).collect() # se obtiene el Id de los usuarios como lista\n    \n\n    daa={'IdBeneficiarion': Ids, 'Reincidencia': rein } #se unen ambas listas el id y el estado de reincidencia\n    dfp=pd.DataFrame(daa)# se crea el dataframe en pandas\n    mySchema = StructType([StructField(\"IdBeneficiarion\", StringType(), True)\\\n                           , StructField(\"Reincidencia\",IntegerType(),True)])# se establece el schema\n\n    dff = spark.createDataFrame(dfp, schema=mySchema)# se crea el dataframe en spark \n\n\n    merged_df=df.join(dff,df.IdBeneficiario ==  dff.IdBeneficiarion,\"inner\") # se hace el join entre el df que entra y el df que contiene el estado de reincidencia bajo la llave del ID \n    merged_df=merged_df.drop(merged_df.IdBeneficiarion) # se elimina la columna duplicada del ID \n    return(merged_df)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9cc588e-d0fc-4fc2-a2e0-26b3f5f5c042"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def balancing(df):# funcion que balancea \n    balancingRatio = df.filter(col(\"Reincidencia\") == 1).count() / df.count()\n    calculateWeights = udf(lambda x: 1 * balancingRatio if x == 0 else (1 * (1.0 - balancingRatio)), DoubleType())\n    weightedDataset = df.withColumn(\"Reincidenciaw\", calculateWeights(\"Reincidencia\"))\n    return(weightedDataset)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"876194ea-a198-4fc6-ab5f-ad468e8681f9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["nulls(raw_datax)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a1378c46-eaf0-4fe4-9451-084cb43cf86f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["poc=filtro4(raw_datax,17,0.8)\npathpoc=\"dbfs:/dbfs/mnt/poc17.parquet\"\npoc.write.mode(\"overwrite\").parquet(pathpoc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0bda8003-6ca7-4682-9fb8-279864e4018d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pathpoc=\"dbfs:/dbfs/mnt/poc17.parquet\"\npoc = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(pathpoc)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74160812-1cf5-460b-9180-e80db230f41e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Balanceo del dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52af51a8-9f01-45b7-92d8-4bded56fba9b"}}},{"cell_type":"code","source":["path=\"dbfs:/dbfs/mnt/p17rfinal.parquet\"\nprueba = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4bd24680-a0a7-4a7d-a712-c8c145b048dc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["\ndef reincidentes(df):# esta funcion devuelve el dataframe de solo los sujetos reincidentes \n    #df=df.select([when(col(c)==\"\",\"Null\").otherwise(col(c)).alias(c) for c in df.columns]) #llena las columnas con null\n    df1=ept(df)\n    df2=reincidenciaff2(df1)\n    df3=df2.filter(df2.Reincidencia==1)\n    df4=df2.filter(df2.Reincidencia==0)\n    return(df3)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae394e37-0a5d-4e16-9bba-a42c9b42c04f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Etapa de String indexer OneHot encoding y vector assembler\n\nObjetivo: volver las variables tipo string a una forma numérica y todas las categoricas hacerles OHE para crear el vector features"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"113ebecf-71b5-4d8f-92ae-b34bdac79ee4"}}},{"cell_type":"code","source":["def features(df): # esta funcion le entra un df y crea un nuevo dataframe con el OHE de las columnas categoricas y devuelve un df con el el vector features que entra a los modelos de ML \n    \n    x=catstring(df)\n    lista=[]\n    listinx=[]\n    for i in x :\n        lista.append(\"num_\"+str(i))# se crea la lista con el nombre de las columnas que se vuelven de string a num\n    y=lista\n    stringindexer_stages =[StringIndexer(inputCols= x, outputCols= y).setHandleInvalid(\"skip\")] # toma todas las columnas que son string \n    listunt=catInt(df) # lista con las columnas que son int \n    #listunt.remove(\"Reincidencia\")\n    listaohc=[]\n    for i in y :# creacion de los nombrede las columnas a las que se les hace OHE\n            listaohc.append(\"vec\"+str(i))\n    \n    onehotencoder_stages = [OneHotEncoder( inputCols=y, outputCols=listaohc, handleInvalid='error')]#, dropLast=True)\n    vectorassembler_stage = VectorAssembler(inputCols=listaohc, outputCol='features')\n    all_stages = stringindexer_stages + onehotencoder_stages + [vectorassembler_stage]\n    pipeline = Pipeline(stages=all_stages).fit(df).transform(df)\n    final_columns = [\"Idbeneficiario\"]+['features'] + ['Reincidenciaw']+['Reincidencia'] #listunt +\n    df_features = pipeline.select(final_columns)\n    return (df_features)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c708ec09-6ac8-4d41-9baa-bd49e5b603ef"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# construccion de las 3 sabanas de datos:\n- La primera con todas las variables \n- La segunda con el 75% de las variables \n- La tercera con el 50% de las variables"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7f45d83-c29d-4ebf-8a2a-4488291311ff"}}},{"cell_type":"code","source":["p1705=filtro4(raw_datax,17,0)\np1005=filtro4(raw_datax,10,0.2)\np605=filtro4(raw_datax,6,0.3)\n\n#creacion de las columnas ept \np17ept=ept(p1705)\np1005ept=ept(p1005)\np605ept=ept(p605)\n\n# creacion de la columna reincidencia\np1705r=reincidenciaff2(p17ept)\np1005eptr=reincidenciaff2(p1005ept)\np605eptr=reincidenciaff2(p605ept)\n\n#arreglar schema categorizando cada una\np1705rcast=castype(p1705r)\np1005rcast=castype(p1005eptr)\np605cast=castype(p605eptr)\n\np1705rcast=balancing(p1705rcast)\np1005rcast=balancing(p1005rcast)\np605cast=balancing(p605cast)\n#Creacion del dataframe final con la columna features que contiene la informacion de todas las columnas \n\n\npath_to_datap1705r=\"dbfs:/dbfs/mnt/p1705r.parquet\"\npath_to_datap605eptr=\"dbfs:/dbfs/mnt/p605eptr.parquet\"\npath_to_datap1005eptr=\"dbfs:/dbfs/mnt/p1005eptr.parquet\"\n\np1705rcast.write.mode(\"overwrite\").parquet(path_to_datap1705r)\np1005rcast.write.mode(\"overwrite\").parquet(path_to_datap605eptr)\np605cast.write.mode(\"overwrite\").parquet(path_to_datap1005eptr)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"872ac42e-2eb6-4ea8-832f-051a4b9e6cb4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path_to_datap1705r=\"dbfs:/dbfs/mnt/p1705r.parquet\"\npath_to_datap605eptr=\"dbfs:/dbfs/mnt/p605eptr.parquet\"\npath_to_datap1005eptr=\"dbfs:/dbfs/mnt/p1005eptr.parquet\"\n\np1705rcast = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_to_datap1705r)\np605cast = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_to_datap605eptr)\np1005rcast = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_to_datap1005eptr)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6887d3a5-f935-4e09-b01c-ee89e78a1bf2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["p17feat=features(p1705rcast)\np6feat=features(p605cast)\np10feat=features(p1005rcast)\n\npath_17feat=\"dbfs:/dbfs/mnt/p17feat.parquet\"\npath_6feat=\"dbfs:/dbfs/mnt/p6feat.parquet\"\npath_10feat=\"dbfs:/dbfs/mnt/p10feat.parquet\"\n\np17feat.write.mode(\"overwrite\").parquet(path_17feat)\np10feat.write.mode(\"overwrite\").parquet(path_10feat)\np6feat.write.mode(\"overwrite\").parquet(path_6feat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aaeb83df-959d-4470-bef2-28710099cb2b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path_17feat=\"dbfs:/dbfs/mnt/p17feat.parquet\"\npath_6feat=\"dbfs:/dbfs/mnt/p6feat.parquet\"\npath_10feat=\"dbfs:/dbfs/mnt/p10feat.parquet\"\n\np17feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_17feat)\npath_6feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_10feat)\np10feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_6feat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6867ccfb-d390-4fe5-8207-65d2c0a43c0a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["print(p1705rcast.count())\nprint(p605cast.count())\nprint(p1005rcast.count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d556f187-b5f9-4426-b18e-18d8b61d08ad"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["nulls(p1705rcast)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"256642e0-ba29-4e6d-8194-569caf3d1617"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["nulls(p605cast)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5c546285-9ff0-4a38-ae16-e7973d98a1f2"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["nulls(p1005rcast)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f10c7902-25e9-4047-ae93-5b2b0af6626b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pocept=ept(poc)\nreinpoc=reincidenciaff2(pocept)\npocreiffcas=castype(reinpoc)\npocbal=balancing(pocreiffcas)\npocfeat=features(pocbal)\ndisplay(pocfeat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f954c595-7bc4-4eac-9551-855172ee2a43"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["%md tres modelos de machine learning \n\n- regresion Lineal\n- Regresion Logistica\n- random Forest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd52eacf-5569-4096-9537-a9f47823dcec"}}},{"cell_type":"code","source":["%sh\n\npip install mlflow\n\n/databricks/python_shell/scripts/PythonShell.py configure"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"024a0c8a-7c4c-463e-a453-41c631c757fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Requirement already satisfied: mlflow in /databricks/python3/lib/python3.8/site-packages (1.27.0)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\nRequirement already satisfied: Flask in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.1.2)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\nRequirement already satisfied: alembic in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.8.0)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: sqlparse&gt;=0.3.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.4.2)\nRequirement already satisfied: protobuf&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\nRequirement already satisfied: gitpython&gt;=2.1.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.27)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.19.2)\nRequirement already satisfied: databricks-cli&gt;=0.8.7 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.17.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.9)\nRequirement already satisfied: requests&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.25.1)\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.1.0)\nRequirement already satisfied: click&gt;=7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (8.1.3)\nRequirement already satisfied: sqlalchemy&gt;=1.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.4.39)\nRequirement already satisfied: gunicorn in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: docker&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (5.0.3)\nRequirement already satisfied: querystring-parser in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: pyyaml&gt;=5.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.0)\nRequirement already satisfied: importlib-metadata!=4.7.0,&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (4.12.0)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\nRequirement already satisfied: prometheus-flask-exporter in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.20.2)\nRequirement already satisfied: oauthlib&gt;=3.1.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow) (3.2.0)\nRequirement already satisfied: tabulate&gt;=0.7.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow) (0.8.10)\nRequirement already satisfied: pyjwt&gt;=1.7.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow) (2.4.0)\nRequirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow) (1.15.0)\nRequirement already satisfied: websocket-client&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from docker&gt;=4.0.0-&gt;mlflow) (1.3.3)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitpython&gt;=2.1.0-&gt;mlflow) (4.0.9)\nRequirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&gt;=2.1.0-&gt;mlflow) (5.0.0)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,&gt;=3.7.0-&gt;mlflow) (3.8.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (4.0.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2.10)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.8/site-packages (from sqlalchemy&gt;=1.4.0-&gt;mlflow) (1.1.2)\nRequirement already satisfied: importlib-resources in /databricks/python3/lib/python3.8/site-packages (from alembic-&gt;mlflow) (5.8.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.8/site-packages (from alembic-&gt;mlflow) (1.2.1)\nRequirement already satisfied: Werkzeug&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Flask-&gt;mlflow) (2.1.2)\nRequirement already satisfied: itsdangerous&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Flask-&gt;mlflow) (2.1.2)\nRequirement already satisfied: Jinja2&gt;=3.0 in /databricks/python3/lib/python3.8/site-packages (from Flask-&gt;mlflow) (3.1.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2&gt;=3.0-&gt;Flask-&gt;mlflow) (2.1.1)\nRequirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn-&gt;mlflow) (52.0.0)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging-&gt;mlflow) (2.4.7)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;mlflow) (2.8.1)\nRequirement already satisfied: prometheus-client in /databricks/python3/lib/python3.8/site-packages (from prometheus-flask-exporter-&gt;mlflow) (0.10.1)\nWARNING: You are using pip version 21.0.1; however, version 22.1.2 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n/databricks/python_shell/scripts/PythonShell.py: line 21: syntax error near unexpected token `&#39;AGG&#39;&#39;\n/databricks/python_shell/scripts/PythonShell.py: line 21: `        mpl.use(&#39;AGG&#39;)&#39;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Requirement already satisfied: mlflow in /databricks/python3/lib/python3.8/site-packages (1.27.0)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\nRequirement already satisfied: Flask in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.1.2)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\nRequirement already satisfied: alembic in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.8.0)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: sqlparse&gt;=0.3.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.4.2)\nRequirement already satisfied: protobuf&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\nRequirement already satisfied: gitpython&gt;=2.1.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.27)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.19.2)\nRequirement already satisfied: databricks-cli&gt;=0.8.7 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.17.0)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.9)\nRequirement already satisfied: requests&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.25.1)\nRequirement already satisfied: cloudpickle in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.1.0)\nRequirement already satisfied: click&gt;=7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (8.1.3)\nRequirement already satisfied: sqlalchemy&gt;=1.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.4.39)\nRequirement already satisfied: gunicorn in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: docker&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (5.0.3)\nRequirement already satisfied: querystring-parser in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: pyyaml&gt;=5.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.0)\nRequirement already satisfied: importlib-metadata!=4.7.0,&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (4.12.0)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\nRequirement already satisfied: prometheus-flask-exporter in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.20.2)\nRequirement already satisfied: oauthlib&gt;=3.1.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow) (3.2.0)\nRequirement already satisfied: tabulate&gt;=0.7.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow) (0.8.10)\nRequirement already satisfied: pyjwt&gt;=1.7.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow) (2.4.0)\nRequirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&gt;=0.8.7-&gt;mlflow) (1.15.0)\nRequirement already satisfied: websocket-client&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from docker&gt;=4.0.0-&gt;mlflow) (1.3.3)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitpython&gt;=2.1.0-&gt;mlflow) (4.0.9)\nRequirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&gt;=2.1.0-&gt;mlflow) (5.0.0)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,&gt;=3.7.0-&gt;mlflow) (3.8.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (4.0.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&gt;=2.17.3-&gt;mlflow) (2.10)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.8/site-packages (from sqlalchemy&gt;=1.4.0-&gt;mlflow) (1.1.2)\nRequirement already satisfied: importlib-resources in /databricks/python3/lib/python3.8/site-packages (from alembic-&gt;mlflow) (5.8.0)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.8/site-packages (from alembic-&gt;mlflow) (1.2.1)\nRequirement already satisfied: Werkzeug&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Flask-&gt;mlflow) (2.1.2)\nRequirement already satisfied: itsdangerous&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Flask-&gt;mlflow) (2.1.2)\nRequirement already satisfied: Jinja2&gt;=3.0 in /databricks/python3/lib/python3.8/site-packages (from Flask-&gt;mlflow) (3.1.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2&gt;=3.0-&gt;Flask-&gt;mlflow) (2.1.1)\nRequirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn-&gt;mlflow) (52.0.0)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging-&gt;mlflow) (2.4.7)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /databricks/python3/lib/python3.8/site-packages (from pandas-&gt;mlflow) (2.8.1)\nRequirement already satisfied: prometheus-client in /databricks/python3/lib/python3.8/site-packages (from prometheus-flask-exporter-&gt;mlflow) (0.10.1)\nWARNING: You are using pip version 21.0.1; however, version 22.1.2 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n/databricks/python_shell/scripts/PythonShell.py: line 21: syntax error near unexpected token `&#39;AGG&#39;&#39;\n/databricks/python_shell/scripts/PythonShell.py: line 21: `        mpl.use(&#39;AGG&#39;)&#39;\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import mlflow\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\nfrom pyspark.ml.evaluation import RegressionEvaluator, BinaryClassificationEvaluator\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\nfrom pyspark.ml.tuning import ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f98a91d-7d5f-4c79-ad92-2bdf16308013"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a10c01b-8445-4d67-990c-f456bc7facdc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/bin/bash: 2: command not found\nCollecting mlflow\n  Downloading mlflow-1.27.0-py3-none-any.whl (17.9 MB)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\nCollecting Flask\n  Downloading Flask-2.1.2-py3-none-any.whl (95 kB)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\nCollecting alembic\n  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\nCollecting sqlparse&gt;=0.3.1\n  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\nRequirement already satisfied: protobuf&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\nCollecting gitpython&gt;=2.1.0\n  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.19.2)\nCollecting databricks-cli&gt;=0.8.7\n  Downloading databricks-cli-0.17.0.tar.gz (81 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.9)\nRequirement already satisfied: requests&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.25.1)\nCollecting cloudpickle\n  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\nCollecting click&gt;=7.0\n  Downloading click-8.1.3-py3-none-any.whl (96 kB)\nCollecting sqlalchemy&gt;=1.4.0\n  Downloading SQLAlchemy-1.4.39-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting gunicorn\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\nCollecting docker&gt;=4.0.0\n  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\nCollecting querystring-parser\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nCollecting pyyaml&gt;=5.1\n  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\nCollecting importlib-metadata!=4.7.0,&gt;=3.7.0\n  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\nCollecting prometheus-flask-exporter\n  Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/bin/bash: 2: command not found\nCollecting mlflow\n  Downloading mlflow-1.27.0-py3-none-any.whl (17.9 MB)\nRequirement already satisfied: pytz in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\nCollecting Flask\n  Downloading Flask-2.1.2-py3-none-any.whl (95 kB)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\nCollecting alembic\n  Downloading alembic-1.8.0-py3-none-any.whl (209 kB)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\nCollecting sqlparse&gt;=0.3.1\n  Downloading sqlparse-0.4.2-py3-none-any.whl (42 kB)\nRequirement already satisfied: protobuf&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\nCollecting gitpython&gt;=2.1.0\n  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.19.2)\nCollecting databricks-cli&gt;=0.8.7\n  Downloading databricks-cli-0.17.0.tar.gz (81 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.9)\nRequirement already satisfied: requests&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.25.1)\nCollecting cloudpickle\n  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\nCollecting click&gt;=7.0\n  Downloading click-8.1.3-py3-none-any.whl (96 kB)\nCollecting sqlalchemy&gt;=1.4.0\n  Downloading SQLAlchemy-1.4.39-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting gunicorn\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\nCollecting docker&gt;=4.0.0\n  Downloading docker-5.0.3-py2.py3-none-any.whl (146 kB)\nCollecting querystring-parser\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nCollecting pyyaml&gt;=5.1\n  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\nCollecting importlib-metadata!=4.7.0,&gt;=3.7.0\n  Downloading importlib_metadata-4.12.0-py3-none-any.whl (21 kB)\nRequirement already satisfied: entrypoints in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\nCollecting prometheus-flask-exporter\n  Downloading prometheus_flask_exporter-0.20.2-py3-none-any.whl (18 kB)\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Cancelled","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["metricName: pyspark.ml.param.Param[MulticlassClassificationEvaluatorMetricType] = Param(parent='undefined', name='metricName', doc='metric name in evaluation (f1|accuracy|weightedPrecision|weightedRecall|weightedTruePositiveRate| weightedFalsePositiveRate|weightedFMeasure|truePositiveRateByLabel| falsePositiveRateByLabel|precisionByLabel|recallByLabel|fMeasureByLabel| logLoss|hammingLoss)')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc34c887-8b7b-4bf1-87f1-fae435a8dfe3"}}},{"cell_type":"markdown","source":["#### Regresión lineal"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a699c7af-4e0e-42bf-b59f-fe06436e7b10"}}},{"cell_type":"code","source":["def reglin(feat):    \n    model_dff1=feat\n    train_df,test_df=model_dff1.randomSplit([0.5,0.5])\n    lin_Reg=LinearRegression(featuresCol='features',labelCol='Reincidencia',weightCol=\"Reincidenciaw\")\n    lrparamGrid = (ParamGridBuilder()\n             .addGrid(lin_Reg.regParam, [ 0.1, 0.5, 1.0, 2.0])\n\n             .addGrid(lin_Reg.elasticNetParam, [0.0, 0.5, 0.75, 1.0])\n\n             .addGrid(lin_Reg.maxIter, [1, 5, 10, 20, 30])\n\n             .build())\n    #evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Reincidencia\", weightCol=\"Reincidenciaw\", metricName=\"rmse\")\n    evaluator = BinaryClassificationEvaluator( labelCol='Reincidencia',weightCol=\"Reincidenciaw\")\n    cv_LiR = CrossValidator(estimator = lin_Reg, \n                      estimatorParamMaps = lrparamGrid,\n                      evaluator = evaluator,\n                      numFolds = 5)\n\n    crossvalidation_mod = cv_LiR.fit(train_df)# fiting cross validation \n    #lrcvSummary = crossvalidation_mod.bestModel.summary # resumen de los parametros de mejor modelo\n    #prediccion en el conjunto de entrenamiento\n    pred_train = crossvalidation_mod.transform(train_df)\n\n    #data frame con la prediccion en el conjunto de test\n    lrpredictions =crossvalidation_mod.transform(test_df) # evaluacion del modelo \n\n    \n    #label_pred_train = pred_train.select('Idbeneficiario','Reincidencia', 'prediction')\n    \n    label_pred_test = lrpredictions.select('Idbeneficiario','Reincidencia','Reincidenciaw', 'prediction')\n\n\n    \n    \n    #Confusion matrix from test data\n    tp = label_pred_test[(lrpredictions.Reincidencia == 1) & (lrpredictions.prediction > 0.5)].count()\n    tn = label_pred_test[(lrpredictions.Reincidencia == 0) & (lrpredictions.prediction< 0.5)].count()\n    fp = label_pred_test[(lrpredictions.Reincidencia == 0) & (lrpredictions.prediction > 0.5)].count()\n    fn = label_pred_test[(lrpredictions.Reincidencia == 1) & (lrpredictions.prediction < 0.5)].count()\n    \n    #indicadores\n    coef=evaluator.evaluate(lrpredictions)\n    r2=evaluator.setMetricName('r2').evaluate(lrpredictions)# R^2\n    coef=print('Intercept: ', crossvalidation_mod.bestModel.intercept)\n    accuracy=float((tp+tn) /(label_pred_test.count()))\n    recall = float(tp)/(tp + fn)\n    precision = float(tp) / (tp +fp)\n    f1=2*(precision*recall/(precision+recall))\n    rmse = RegressionEvaluator(labelCol=\"Reincidenciaw\", predictionCol=\"prediction\", metricName=\"rmse\")\n    rmse = rmse.evaluate(label_pred_test)\n    score = {\"tp\":tp,\"tn\":tn,\"fp\":fp,\"fn\":fn,\"accuracy\": accuracy, \"recall\": recall, \"precision\": precision,\"f1\":f1}\n    mae = RegressionEvaluator(labelCol=\"Reincidencia\", predictionCol=\"prediction\", metricName=\"mae\")\n    mae = mae.evaluate(label_pred_test)\n    r2 = RegressionEvaluator(labelCol=\"Reincidencia\", predictionCol=\"prediction\", metricName=\"r2\")\n    r2 = r2.evaluate(label_pred_test)\n\n\n    print(\"MAE: \", mae)\n    print(\"R-squared: \", r2)\n    print(score)\n    print('RMSE:',rmse)\n    \n    return(label_pred_test)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"713b6258-ff9f-4635-af9a-b4c986e2ae7f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["reglog(pocfeat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bcc2f8a2-b05f-4437-bd68-e17b85ce885c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path_17feat=\"dbfs:/dbfs/mnt/p17feat.parquet\"\np17feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_17feat)\np17reglin=reglin(p17feat)\npath_17reglin=\"dbfs:/dbfs/mnt/p17reglin.parquet\"\np17reglin.write.mode(\"overwrite\").parquet(path_17reglin)\n\npath_6feat=\"dbfs:/dbfs/mnt/p6feat.parquet\"\np6feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_10feat)\np6reglin=reglin(p6feat)\npath_6reglin=\"dbfs:/dbfs/mnt/p6reglin.parquet\"\np6reglin.write.mode(\"overwrite\").parquet(path_6reglin)\n\npath_10feat=\"dbfs:/dbfs/mnt/p10feat.parquet\"\np10feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_6feat)\np10reglin=reglin(p10feat)\npath_10reglin=\"dbfs:/dbfs/mnt/p10reglin.parquet\"\np10reglin.write.mode(\"overwrite\").parquet(path_10reglin)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2ea10a57-f829-4a92-aa64-90add717cdf5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:894: UserWarning: MLlib could not reach the MLflow server at tracking URI: databricks\nException: You haven&#39;t configured the CLI yet! Please configure by entering `/databricks/python_shell/scripts/PythonShell.py configure`\n  warnings.warn(\n/databricks/spark/python/pyspark/ml/util.py:92: UserWarning: CrossValidator_8c6c1f484430 fit call failed but some spark jobs may still running for unfinished trials. To address this issue, you should enable pyspark pinned thread mode.\n  warnings.warn(&#34;{} fit call failed but some spark jobs &#34;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:894: UserWarning: MLlib could not reach the MLflow server at tracking URI: databricks\nException: You haven&#39;t configured the CLI yet! Please configure by entering `/databricks/python_shell/scripts/PythonShell.py configure`\n  warnings.warn(\n/databricks/spark/python/pyspark/ml/util.py:92: UserWarning: CrossValidator_8c6c1f484430 fit call failed but some spark jobs may still running for unfinished trials. To address this issue, you should enable pyspark pinned thread mode.\n  warnings.warn(&#34;{} fit call failed but some spark jobs &#34;\n</div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3428115858088589&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> path_17feat<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;dbfs:/dbfs/mnt/p17feat.parquet&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> p17feat <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span>inferSchema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;True&#39;</span><span class=\"ansi-blue-fg\">,</span>header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;True&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;parquet&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>path_17feat<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>p17reglin<span class=\"ansi-blue-fg\">=</span>reglin<span class=\"ansi-blue-fg\">(</span>p17feat<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> path_17reglin<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;dbfs:/dbfs/mnt/p17reglin.parquet&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> p17reglin<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;overwrite&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>path_17reglin<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-3410559751730731&gt;</span> in <span class=\"ansi-cyan-fg\">reglin</span><span class=\"ansi-blue-fg\">(feat)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     18</span>                       numFolds = 5)\n<span class=\"ansi-green-intense-fg ansi-bold\">     19</span> \n<span class=\"ansi-green-fg\">---&gt; 20</span><span class=\"ansi-red-fg\">     </span>crossvalidation_mod <span class=\"ansi-blue-fg\">=</span> cv_LiR<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>train_df<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"># fiting cross validation</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     21</span>     <span class=\"ansi-red-fg\">#lrcvSummary = crossvalidation_mod.bestModel.summary # resumen de los parametros de mejor modelo</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     22</span>     <span class=\"ansi-red-fg\">#prediccion en el conjunto de entrenamiento</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/tuning.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    711</span>                         subModels<span class=\"ansi-blue-fg\">[</span>i<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span>j<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> subModel\n<span class=\"ansi-green-intense-fg ansi-bold\">    712</span> \n<span class=\"ansi-green-fg\">--&gt; 713</span><span class=\"ansi-red-fg\">             </span>_cancel_on_failure<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>uid<span class=\"ansi-blue-fg\">,</span> sub_task_failed<span class=\"ansi-blue-fg\">,</span> calculate_metrics<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    714</span>             <span class=\"ansi-red-fg\"># END-EDGE</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    715</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/util.py</span> in <span class=\"ansi-cyan-fg\">_cancel_on_failure</span><span class=\"ansi-blue-fg\">(sc, uid, sub_task_failed, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     94</span>                           <span class=\"ansi-blue-fg\">&#34;issue, you should enable pyspark pinned thread mode.&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     95</span>                           .format(uid))\n<span class=\"ansi-green-fg\">---&gt; 96</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">raise</span> e\n<span class=\"ansi-green-intense-fg ansi-bold\">     97</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     98</span>     old_job_group <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>getLocalProperty<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;spark.jobGroup.id&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/util.py</span> in <span class=\"ansi-cyan-fg\">_cancel_on_failure</span><span class=\"ansi-blue-fg\">(sc, uid, sub_task_failed, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     88</span>     <span class=\"ansi-green-fg\">if</span> os<span class=\"ansi-blue-fg\">.</span>environ<span class=\"ansi-blue-fg\">.</span>get<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;PYSPARK_PIN_THREAD&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;false&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>lower<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">!=</span> <span class=\"ansi-blue-fg\">&#34;true&#34;</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     89</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 90</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     91</span>         <span class=\"ansi-green-fg\">except</span> Exception <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     92</span>             warnings.warn(&#34;{} fit call failed but some spark jobs &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/util.py</span> in <span class=\"ansi-cyan-fg\">wrapper</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    148</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    149</span>             ipython<span class=\"ansi-blue-fg\">.</span>events<span class=\"ansi-blue-fg\">.</span>register<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;post_run_cell&#34;</span><span class=\"ansi-blue-fg\">,</span> on_cancel<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 150</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    151</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    152</span>         <span class=\"ansi-green-fg\">return</span> wrapper\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/tuning.py</span> in <span class=\"ansi-cyan-fg\">calculate_metrics</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    705</span>                     <span class=\"ansi-green-fg\">return</span> task<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    706</span> \n<span class=\"ansi-green-fg\">--&gt; 707</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">for</span> j<span class=\"ansi-blue-fg\">,</span> metric<span class=\"ansi-blue-fg\">,</span> subModel <span class=\"ansi-green-fg\">in</span> pool<span class=\"ansi-blue-fg\">.</span>imap_unordered<span class=\"ansi-blue-fg\">(</span>run_task<span class=\"ansi-blue-fg\">,</span> tasks<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    708</span>                     metrics<span class=\"ansi-blue-fg\">[</span>j<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-blue-fg\">(</span>metric <span class=\"ansi-blue-fg\">/</span> nFolds<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    709</span>                     metrics_all<span class=\"ansi-blue-fg\">[</span>i<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span>j<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> metric\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/multiprocessing/pool.py</span> in <span class=\"ansi-cyan-fg\">next</span><span class=\"ansi-blue-fg\">(self, timeout)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    866</span>         <span class=\"ansi-green-fg\">if</span> success<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    867</span>             <span class=\"ansi-green-fg\">return</span> value\n<span class=\"ansi-green-fg\">--&gt; 868</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> value\n<span class=\"ansi-green-intense-fg ansi-bold\">    869</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    870</span>     __next__ <span class=\"ansi-blue-fg\">=</span> next                    <span class=\"ansi-red-fg\"># XXX</span>\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/multiprocessing/pool.py</span> in <span class=\"ansi-cyan-fg\">worker</span><span class=\"ansi-blue-fg\">(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    123</span>         job<span class=\"ansi-blue-fg\">,</span> i<span class=\"ansi-blue-fg\">,</span> func<span class=\"ansi-blue-fg\">,</span> args<span class=\"ansi-blue-fg\">,</span> kwds <span class=\"ansi-blue-fg\">=</span> task\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 125</span><span class=\"ansi-red-fg\">             </span>result <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> func<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwds<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">except</span> Exception <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    127</span>             <span class=\"ansi-green-fg\">if</span> wrap_exception <span class=\"ansi-green-fg\">and</span> func <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> _helper_reraises_exception<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/tuning.py</span> in <span class=\"ansi-cyan-fg\">run_task</span><span class=\"ansi-blue-fg\">(task)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    703</span>                     <span class=\"ansi-green-fg\">if</span> sub_task_failed<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    704</span>                         <span class=\"ansi-green-fg\">raise</span> RuntimeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Terminate this task because one of other task failed.&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 705</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">return</span> task<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    706</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    707</span>                 <span class=\"ansi-green-fg\">for</span> j<span class=\"ansi-blue-fg\">,</span> metric<span class=\"ansi-blue-fg\">,</span> subModel <span class=\"ansi-green-fg\">in</span> pool<span class=\"ansi-blue-fg\">.</span>imap_unordered<span class=\"ansi-blue-fg\">(</span>run_task<span class=\"ansi-blue-fg\">,</span> tasks<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/tuning.py</span> in <span class=\"ansi-cyan-fg\">singleTask</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     73</span>         <span class=\"ansi-red-fg\">#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     74</span>         <span class=\"ansi-red-fg\">#  all nested stages and evaluators</span>\n<span class=\"ansi-green-fg\">---&gt; 75</span><span class=\"ansi-red-fg\">         </span>metric <span class=\"ansi-blue-fg\">=</span> eva<span class=\"ansi-blue-fg\">.</span>evaluate<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>validation<span class=\"ansi-blue-fg\">,</span> epm<span class=\"ansi-blue-fg\">[</span>index<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     76</span>         <span class=\"ansi-green-fg\">return</span> index<span class=\"ansi-blue-fg\">,</span> metric<span class=\"ansi-blue-fg\">,</span> model <span class=\"ansi-green-fg\">if</span> collectSubModel <span class=\"ansi-green-fg\">else</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     77</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/evaluation.py</span> in <span class=\"ansi-cyan-fg\">evaluate</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     82</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     83</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 84</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     85</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     86</span>             <span class=\"ansi-green-fg\">raise</span> ValueError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Params must be a param map but got %s.&#34;</span> <span class=\"ansi-blue-fg\">%</span> type<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/evaluation.py</span> in <span class=\"ansi-cyan-fg\">_evaluate</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 120</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>     <span class=\"ansi-green-fg\">def</span> isLargerBetter<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: rawPrediction does not exist. Available: Idbeneficiario, features, Reincidenciaw, Reincidencia, CrossValidator_8c6c1f484430_rand, prediction</div>","errorSummary":"<span class=\"ansi-red-fg\">IllegalArgumentException</span>: rawPrediction does not exist. Available: Idbeneficiario, features, Reincidenciaw, Reincidencia, CrossValidator_8c6c1f484430_rand, prediction","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>                  Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-3428115858088589&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      1</span> path_17feat<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;dbfs:/dbfs/mnt/p17feat.parquet&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      2</span> p17feat <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>read<span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span>inferSchema<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;True&#39;</span><span class=\"ansi-blue-fg\">,</span>header<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#39;True&#39;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;parquet&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>load<span class=\"ansi-blue-fg\">(</span>path_17feat<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">----&gt; 3</span><span class=\"ansi-red-fg\"> </span>p17reglin<span class=\"ansi-blue-fg\">=</span>reglin<span class=\"ansi-blue-fg\">(</span>p17feat<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> path_17reglin<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-blue-fg\">&#34;dbfs:/dbfs/mnt/p17reglin.parquet&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> p17reglin<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;overwrite&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>parquet<span class=\"ansi-blue-fg\">(</span>path_17reglin<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">&lt;command-3410559751730731&gt;</span> in <span class=\"ansi-cyan-fg\">reglin</span><span class=\"ansi-blue-fg\">(feat)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     18</span>                       numFolds = 5)\n<span class=\"ansi-green-intense-fg ansi-bold\">     19</span> \n<span class=\"ansi-green-fg\">---&gt; 20</span><span class=\"ansi-red-fg\">     </span>crossvalidation_mod <span class=\"ansi-blue-fg\">=</span> cv_LiR<span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>train_df<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-red-fg\"># fiting cross validation</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     21</span>     <span class=\"ansi-red-fg\">#lrcvSummary = crossvalidation_mod.bestModel.summary # resumen de los parametros de mejor modelo</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     22</span>     <span class=\"ansi-red-fg\">#prediccion en el conjunto de entrenamiento</span>\n\n<span class=\"ansi-green-fg\">/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py</span> in <span class=\"ansi-cyan-fg\">patched_method</span><span class=\"ansi-blue-fg\">(self, *args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     28</span>             call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">False</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     29</span>             <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 30</span><span class=\"ansi-red-fg\">                 </span>result <span class=\"ansi-blue-fg\">=</span> original_method<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     31</span>                 call_succeeded <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-green-fg\">True</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     32</span>                 <span class=\"ansi-green-fg\">return</span> result\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/base.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    159</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    160</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 161</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_fit<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    162</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    163</span>             raise ValueError(&#34;Params must be either a param map or a list/tuple of param maps, &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/tuning.py</span> in <span class=\"ansi-cyan-fg\">_fit</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    711</span>                         subModels<span class=\"ansi-blue-fg\">[</span>i<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span>j<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> subModel\n<span class=\"ansi-green-intense-fg ansi-bold\">    712</span> \n<span class=\"ansi-green-fg\">--&gt; 713</span><span class=\"ansi-red-fg\">             </span>_cancel_on_failure<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_sc<span class=\"ansi-blue-fg\">,</span> self<span class=\"ansi-blue-fg\">.</span>uid<span class=\"ansi-blue-fg\">,</span> sub_task_failed<span class=\"ansi-blue-fg\">,</span> calculate_metrics<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    714</span>             <span class=\"ansi-red-fg\"># END-EDGE</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    715</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/util.py</span> in <span class=\"ansi-cyan-fg\">_cancel_on_failure</span><span class=\"ansi-blue-fg\">(sc, uid, sub_task_failed, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     94</span>                           <span class=\"ansi-blue-fg\">&#34;issue, you should enable pyspark pinned thread mode.&#34;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     95</span>                           .format(uid))\n<span class=\"ansi-green-fg\">---&gt; 96</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">raise</span> e\n<span class=\"ansi-green-intense-fg ansi-bold\">     97</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     98</span>     old_job_group <span class=\"ansi-blue-fg\">=</span> sc<span class=\"ansi-blue-fg\">.</span>getLocalProperty<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;spark.jobGroup.id&#34;</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/util.py</span> in <span class=\"ansi-cyan-fg\">_cancel_on_failure</span><span class=\"ansi-blue-fg\">(sc, uid, sub_task_failed, f)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     88</span>     <span class=\"ansi-green-fg\">if</span> os<span class=\"ansi-blue-fg\">.</span>environ<span class=\"ansi-blue-fg\">.</span>get<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;PYSPARK_PIN_THREAD&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;false&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>lower<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-blue-fg\">!=</span> <span class=\"ansi-blue-fg\">&#34;true&#34;</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     89</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 90</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     91</span>         <span class=\"ansi-green-fg\">except</span> Exception <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     92</span>             warnings.warn(&#34;{} fit call failed but some spark jobs &#34;\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/util.py</span> in <span class=\"ansi-cyan-fg\">wrapper</span><span class=\"ansi-blue-fg\">(*args, **kwargs)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    148</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    149</span>             ipython<span class=\"ansi-blue-fg\">.</span>events<span class=\"ansi-blue-fg\">.</span>register<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;post_run_cell&#34;</span><span class=\"ansi-blue-fg\">,</span> on_cancel<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 150</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwargs<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    151</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    152</span>         <span class=\"ansi-green-fg\">return</span> wrapper\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/tuning.py</span> in <span class=\"ansi-cyan-fg\">calculate_metrics</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    705</span>                     <span class=\"ansi-green-fg\">return</span> task<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    706</span> \n<span class=\"ansi-green-fg\">--&gt; 707</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">for</span> j<span class=\"ansi-blue-fg\">,</span> metric<span class=\"ansi-blue-fg\">,</span> subModel <span class=\"ansi-green-fg\">in</span> pool<span class=\"ansi-blue-fg\">.</span>imap_unordered<span class=\"ansi-blue-fg\">(</span>run_task<span class=\"ansi-blue-fg\">,</span> tasks<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    708</span>                     metrics<span class=\"ansi-blue-fg\">[</span>j<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">+=</span> <span class=\"ansi-blue-fg\">(</span>metric <span class=\"ansi-blue-fg\">/</span> nFolds<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    709</span>                     metrics_all<span class=\"ansi-blue-fg\">[</span>i<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">[</span>j<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">=</span> metric\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/multiprocessing/pool.py</span> in <span class=\"ansi-cyan-fg\">next</span><span class=\"ansi-blue-fg\">(self, timeout)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    866</span>         <span class=\"ansi-green-fg\">if</span> success<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    867</span>             <span class=\"ansi-green-fg\">return</span> value\n<span class=\"ansi-green-fg\">--&gt; 868</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">raise</span> value\n<span class=\"ansi-green-intense-fg ansi-bold\">    869</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    870</span>     __next__ <span class=\"ansi-blue-fg\">=</span> next                    <span class=\"ansi-red-fg\"># XXX</span>\n\n<span class=\"ansi-green-fg\">/usr/lib/python3.8/multiprocessing/pool.py</span> in <span class=\"ansi-cyan-fg\">worker</span><span class=\"ansi-blue-fg\">(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    123</span>         job<span class=\"ansi-blue-fg\">,</span> i<span class=\"ansi-blue-fg\">,</span> func<span class=\"ansi-blue-fg\">,</span> args<span class=\"ansi-blue-fg\">,</span> kwds <span class=\"ansi-blue-fg\">=</span> task\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 125</span><span class=\"ansi-red-fg\">             </span>result <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">(</span><span class=\"ansi-green-fg\">True</span><span class=\"ansi-blue-fg\">,</span> func<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>args<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kwds<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">except</span> Exception <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    127</span>             <span class=\"ansi-green-fg\">if</span> wrap_exception <span class=\"ansi-green-fg\">and</span> func <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> _helper_reraises_exception<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/tuning.py</span> in <span class=\"ansi-cyan-fg\">run_task</span><span class=\"ansi-blue-fg\">(task)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    703</span>                     <span class=\"ansi-green-fg\">if</span> sub_task_failed<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">0</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    704</span>                         <span class=\"ansi-green-fg\">raise</span> RuntimeError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Terminate this task because one of other task failed.&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 705</span><span class=\"ansi-red-fg\">                     </span><span class=\"ansi-green-fg\">return</span> task<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    706</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    707</span>                 <span class=\"ansi-green-fg\">for</span> j<span class=\"ansi-blue-fg\">,</span> metric<span class=\"ansi-blue-fg\">,</span> subModel <span class=\"ansi-green-fg\">in</span> pool<span class=\"ansi-blue-fg\">.</span>imap_unordered<span class=\"ansi-blue-fg\">(</span>run_task<span class=\"ansi-blue-fg\">,</span> tasks<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/tuning.py</span> in <span class=\"ansi-cyan-fg\">singleTask</span><span class=\"ansi-blue-fg\">()</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     73</span>         <span class=\"ansi-red-fg\">#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     74</span>         <span class=\"ansi-red-fg\">#  all nested stages and evaluators</span>\n<span class=\"ansi-green-fg\">---&gt; 75</span><span class=\"ansi-red-fg\">         </span>metric <span class=\"ansi-blue-fg\">=</span> eva<span class=\"ansi-blue-fg\">.</span>evaluate<span class=\"ansi-blue-fg\">(</span>model<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>validation<span class=\"ansi-blue-fg\">,</span> epm<span class=\"ansi-blue-fg\">[</span>index<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     76</span>         <span class=\"ansi-green-fg\">return</span> index<span class=\"ansi-blue-fg\">,</span> metric<span class=\"ansi-blue-fg\">,</span> model <span class=\"ansi-green-fg\">if</span> collectSubModel <span class=\"ansi-green-fg\">else</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     77</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/evaluation.py</span> in <span class=\"ansi-cyan-fg\">evaluate</span><span class=\"ansi-blue-fg\">(self, dataset, params)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     82</span>                 <span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>copy<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>_evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     83</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 84</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     85</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     86</span>             <span class=\"ansi-green-fg\">raise</span> ValueError<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;Params must be a param map but got %s.&#34;</span> <span class=\"ansi-blue-fg\">%</span> type<span class=\"ansi-blue-fg\">(</span>params<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/ml/evaluation.py</span> in <span class=\"ansi-cyan-fg\">_evaluate</span><span class=\"ansi-blue-fg\">(self, dataset)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>         self<span class=\"ansi-blue-fg\">.</span>_transfer_params_to_java<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 120</span><span class=\"ansi-red-fg\">         </span><span class=\"ansi-green-fg\">return</span> self<span class=\"ansi-blue-fg\">.</span>_java_obj<span class=\"ansi-blue-fg\">.</span>evaluate<span class=\"ansi-blue-fg\">(</span>dataset<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>     <span class=\"ansi-green-fg\">def</span> isLargerBetter<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    121</span>                 <span class=\"ansi-red-fg\"># Hide where the exception came from that shows a non-Pythonic</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    122</span>                 <span class=\"ansi-red-fg\"># JVM exception message.</span>\n<span class=\"ansi-green-fg\">--&gt; 123</span><span class=\"ansi-red-fg\">                 </span><span class=\"ansi-green-fg\">raise</span> converted <span class=\"ansi-green-fg\">from</span> <span class=\"ansi-green-fg\">None</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    124</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>                 <span class=\"ansi-green-fg\">raise</span>\n\n<span class=\"ansi-red-fg\">IllegalArgumentException</span>: rawPrediction does not exist. Available: Idbeneficiario, features, Reincidenciaw, Reincidencia, CrossValidator_8c6c1f484430_rand, prediction</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Regresion Logística"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c25d84a-2dd3-44a1-8176-70998a95ccf0"}}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\ndef reglogist(feat):\n    #se toman solo las columnas features y reincidencia balanceado\n    model_df=feat\n    \n    #divison aleatoria\n    training_df,test_df=model_df.randomSplit([0.5,0.5]) \n    \n    #aplicacion de la regresion logistica \n    log_reg = LogisticRegression(featuresCol='features',labelCol='Reincidencia',weightCol=\"Reincidenciaw\")\n    \n    #modelo al set de entranmiento\n    #log_reg.fit(training_df)#reincidencia\n    \n    # evaluador de desempeño binario \n    #evaluator_LR = BinaryClassificationEvaluator( labelCol='probability')\n    evaluator_LR = BinaryClassificationEvaluator( labelCol='Reincidencia',weightCol=\"Reincidenciaw\")\n    \n    # Creacion ParamGrid para Cross Validation\n    paramGrid_LR = (ParamGridBuilder()\n                 .addGrid(log_reg.regParam, [ 0.1, 0.5, 1.0, 2.0])\n                 .addGrid(log_reg.elasticNetParam, [0.0, 0.5, 0.75, 1.0])\n                 .addGrid(log_reg.maxIter, [1, 5, 10, 20, 40])\n                 .build())\n\n\n    pipelinelr = Pipeline(stages=[log_reg])\n\n    # Crea 5-fold CrossValidator\n    cv_LR = CrossValidator(estimator = pipelinelr,\n                          estimatorParamMaps = paramGrid_LR,\n                          evaluator = evaluator_LR,\n                          numFolds = 5)\n    \n    \n    # Fit cross-validation model Run cross validations.\n    cvModel_lr = cv_LR.fit(model_df)\n    \n    #Prediction\n    \n    #Prediction on training data\n    pred_train = cvModel_lr.transform(training_df)\n    \n    #Prediction on test data\n    predictions_lr = cvModel_lr.transform(test_df)\n    \n    lrcvSummary = cvModel_lr.bestModel.summary\n    \n    #evaluacion del entrenamiento de la prediccion \n    #train_results=log_reg.evaluate(training_df).predictions#reincidencia\n\n    # evaluacion del modelo\n    #results=log_reg.evaluate(test_df).predictions#reincidencia\n    \n    \n    \n    \n    \n    #Confusion matrix from test data\n\n    \n    label_pred_test = predictions_lr.select('Idbeneficiario','Reincidencia','Reincidenciaw', 'prediction')\n    \n    \n    # Evaluacion del  modelo indicadores\n    \n    tp = label_pred_test[(predictions_lr.Reincidencia == 1) & (predictions_lr.prediction > 0.5)].count()\n    tn = label_pred_test[(predictions_lr.Reincidencia == 0) & (predictions_lr.prediction< 0.5)].count()\n    fp = label_pred_test[(predictions_lr.Reincidencia == 0) & (predictions_lr.prediction > 0.5)].count()\n    fn = label_pred_test[(predictions_lr.Reincidencia == 1) & (predictions_lr.prediction < 0.5)].count()\n    \n    accuracy=float((tp+tn) /(label_pred_test.count()))\n    recall = float(tp)/(tp + fn)\n    precision = float(tp) / (tp +fp)\n    f1=2*(precision*recall/(precision+recall))\n    \n    ROCTest=evaluator.setMetricName('areaUnderROC').evaluate(label_pred_test)\n    rms1=evaluator.setMetricName('rmse').evaluate(label_pred_test)\n    mae1=evaluator.setMetricName('mae').evaluate(label_pred_test)\n    r21=evaluator.setMetricName('r2').evaluate(label_pred_test)\n    print(\"AUROC:\",ROCTest)\n    print(\"MAE: \", mae1)\n    print(\"R-squared: \", r21)\n    print('RMSE:',rms1)\n    \n    \n    rmse = RegressionEvaluator(labelCol=\"Reincidenciaw\", predictionCol=\"prediction\", metricName=\"rmse\")\n    rmse = rmse.evaluate(label_pred_test)\n    score = {\"tp\":tp,\"tn\":tn,\"fp\":fp,\"fn\":fn,\"accuracy\": accuracy, \"recall\": recall, \"precision\": precision,\"f1\":f1}\n    mae = RegressionEvaluator(labelCol=\"Reincidencia\", predictionCol=\"prediction\", metricName=\"mae\")\n    mae = mae.evaluate(label_pred_test)\n    r2 = RegressionEvaluator(labelCol=\"Reincidencia\", predictionCol=\"prediction\", metricName=\"r2\")\n    r2 = r2.evaluate(label_pred_test)\n    areaUnderROCtest = RegressionEvaluator(labelCol=\"Reincidenciaw\", predictionCol=\"prediction\", metricName=\"areaUnderROC\")\n    areaUnderROCtest = areaUnderROCtest.evaluate(label_pred_test)\n    \n    print(\"AUROC:\",areaUnderROCtest)\n    print(\"MAE: \", mae)\n    print(\"R-squared: \", r2)\n    print(score)\n    print('RMSE:',rmse)\n    \n    return(label_pred_test)\n\n\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"079754b6-95b5-4b67-bc58-138d20bebd90"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["reglogist(pocfeat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d83f99e4-e27c-48ac-94b1-c569ba1b2940"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["path_17feat=\"dbfs:/dbfs/mnt/p17feat.parquet\"\np17feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_17feat)\np17logreg=reglogist(p17feat)\npath_17logreg=\"dbfs:/dbfs/mnt/p17logreg.parquet\"\np17logreg.write.mode(\"overwrite\").parquet(path_17logreg)\n\npath_6feat=\"dbfs:/dbfs/mnt/p6feat.parquet\"\np6feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_10feat)\np6logreg=reglogist(p6feat)\npath_6logreg=\"dbfs:/dbfs/mnt/p6logreg.parquet\"\np6logreg.write.mode(\"overwrite\").parquet(path_6logreg)\n\npath_10feat=\"dbfs:/dbfs/mnt/p10feat.parquet\"\np10feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_6feat)\np10logreg=reglogist(p10feat)\npath_10logreg=\"dbfs:/dbfs/mnt/p10logreg.parquet\"\np10logreg.write.mode(\"overwrite\").parquet(path_10logreg)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07fea81b-e4f7-4ce9-9ab1-4fabef2d8449"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### Random Forest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9732604d-a1eb-4b30-bac1-ca631df11561"}}},{"cell_type":"code","source":["def randomforest(feat):\n    model_RF=feat\n    \n    #division aleatoria \n    train_df,tes_df=model_df.randomSplit([0.5,0.5])\n    \n    #estimador\n    random_forest = RandomForestClassifier(featuresCol='features',labelCol='Reincidencia',weightCol=\"Reincidenciaw\")\n    \n    \n    # Evaluate model\n    rfevaluator = BinaryClassificationEvaluator( labelCol='Reincidencia',weightCol=\"Reincidenciaw\")\n    #evaluator = BinaryClassificationEvaluator()\n    \n    # Creacion grid de parametrospara Cross Validation\n\n    pipeline = Pipeline(stages=[rf_classifier])\n\n    rfparamGrid = (ParamGridBuilder()\n                   .addGrid(random_forest.maxDepth, [2, 5, 10, 20, 30])\n                   .addGrid(random_forest.maxBins, [5, 10, 20, 40, 100])\n                   .addGrid(random_forest.numTrees, [5, 10, 20, 40, 50])\n                   .addGrid(random_forest.minInfoGain, [0.0, 0.1, 0.2, 0.3])\n                 .build())\n\n\n\n    #  5-fold CrossValidator\n    rfcv = CrossValidator(estimator = pipeline,\n                          estimatorParamMaps = rfparamGrid,\n                          evaluator = rfevaluator,\n                          numFolds = 5)\n    \n    #Fit cross-validation model\n    crossvalidation_mod = rfcv.fit(model_RF)\n    \n    #prediction\n    \n    #en training\n    pred_train = crossvalidation_mod.transform(train_df)\n    \n    #en test\n    pred_test = crossvalidation_mod.transform(tes_df)\n    \n    label_pred_test = pred_test.select('Idbeneficiario','Reincidencia','Reincidenciaw', 'prediction')\n    \n    #Prediction performance\n    \n    # Evaluacion del  modelo indicadores\n    \n    tp = label_pred_test[(predictions_lr.Reincidencia == 1) & (predictions_lr.prediction > 0.5)].count()\n    tn = label_pred_test[(predictions_lr.Reincidencia == 0) & (predictions_lr.prediction< 0.5)].count()\n    fp = label_pred_test[(predictions_lr.Reincidencia == 0) & (predictions_lr.prediction > 0.5)].count()\n    fn = label_pred_test[(predictions_lr.Reincidencia == 1) & (predictions_lr.prediction < 0.5)].count()\n    \n    \n    #ROC\n    \n    ROCTest=evaluator.setMetricName('areaUnderROC').evaluate(label_pred_test)\n    rms1=evaluator.setMetricName('rmse').evaluate(label_pred_test)\n    mae1=evaluator.setMetricName('mae').evaluate(label_pred_test)\n    r21=evaluator.setMetricName('r2').evaluate(label_pred_test)\n    print(\"AUROC:\",ROCTest)\n    print(\"MAE: \", mae1)\n    print(\"R-squared: \", r21)\n    print('RMSE:',rms1)\n    \n    accuracy=float((tp+tn) /(label_pred_test.count()))\n    recall = float(tp)/(tp + fn)\n    precision = float(tp) / (tp +fp)\n    f1=2*(precision*recall/(precision+recall))\n    rmse = RegressionEvaluator(labelCol=\"Reincidenciaw\", predictionCol=\"prediction\", metricName=\"rmse\")\n    rmse = rmse.evaluate(label_pred_test)\n    score = {\"tp\":tp,\"tn\":tn,\"fp\":fp,\"fn\":fn,\"accuracy\": accuracy, \"recall\": recall, \"precision\": precision,\"f1\":f1}\n    mae = RegressionEvaluator(labelCol=\"Reincidencia\", predictionCol=\"prediction\", metricName=\"mae\")\n    mae = mae.evaluate(label_pred_test)\n    r2 = RegressionEvaluator(labelCol=\"Reincidencia\", predictionCol=\"prediction\", metricName=\"r2\")\n    r2 = r2.evaluate(label_pred_test)\n    areaUnderROCtest = RegressionEvaluator(labelCol=\"Reincidenciaw\", predictionCol=\"prediction\", metricName=\"areaUnderROC\")\n    areaUnderROCtest = areaUnderROCtest.evaluate(label_pred_test)\n    \n    print(\"AUROC:\",areaUnderROCtest)\n    print(\"MAE: \", mae)\n    print(\"R-squared: \", r2)\n    print('RMSE:',rmse)\n    print(score)\n    \n    return(label_pred_test)\n    \n    \n    \n    \n   \n    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f033a884-e630-4a74-bd0a-395512a2560d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["path_17feat=\"dbfs:/dbfs/mnt/p17feat.parquet\"\np17feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_17feat)\np17rf=randomforest(p17feat)\npath_17rf=\"dbfs:/dbfs/mnt/p17rf.parquet\"\np17rf.write.mode(\"overwrite\").parquet(path_17rf)\n\npath_6feat=\"dbfs:/dbfs/mnt/p6feat.parquet\"\np6feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_10feat)\np6rf=randomforest(p6feat)\npath_6rf=\"dbfs:/dbfs/mnt/p6rf.parquet\"\np6rf.write.mode(\"overwrite\").parquet(path_6rf)\n\npath_10feat=\"dbfs:/dbfs/mnt/p10feat.parquet\"\np10feat = spark.read.options(inferSchema='True',header='True').format(\"parquet\").load(path_6feat)\np10rf=randomforest(p10feat)\npath_10rf=\"dbfs:/dbfs/mnt/p10rf.parquet\"\np10rf.write.mode(\"overwrite\").parquet(path_10rf)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40a73b38-36a1-4ff2-a910-16a526323313"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["randomforest(pocfeat)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b06b7fe6-b3c8-4594-b719-6919f867b214"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["random_forest.write().overwrite().save('dbfs:/mnt/gold/icbf-gai/Random_Forest_Classifier/') # asi se guarda el modelo final"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae7f0b29-39d3-452e-807b-a73cf25c446b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark2=SparkSession.builder.appName('random_forest').getOrCreate()\nmodel_df4=p1705_df.select(['IdBeneficiario','features','Reincidencia'])\n\n#division aleatoria \ntrain_df,tes_df=model_df.randomSplit([0.75,0.25])#EPT3\n\n#aplicacion del arbol aleatorio \nrf_classifier=RandomForestClassifier(labelCol='Reincidencia',numTrees=50).fit(train_df)\n\n#transformacion del clasificadoer \nrf_predictions=rf_classifier.transform(tes_df)\n#evaluacion\nresultsrf=rf_classifier.evaluate(tes_df).predictions\n# evaluacion del acuracy\nrf_accuracy=MulticlassClassificationEvaluator(labelCol='Reincidencia',metricName='accuracy').evaluate(rf_predictions)\n\n#evaluacion de la precision ponderada\nrf_precision=MulticlassClassificationEvaluator(labelCol='Reincidencia',metricName='weightedPrecision').evaluate(rf_predictions)\n#AUC\nrf_auc=BinaryClassificationEvaluator(rawPredictionCol=resultsrf, labelCol= 'Reincidencia', metricName = 'areaUnderROC',).evaluate(rf_predictions)\n \n\n#Metrics\nprint('El Accuracy Score con los datos de test es {0:.0%}'.format(rf_accuracy))\nprint('El Precision Rate Score con los datos de test es {0:.0%}'.format(rf_precision))\nprint('El AUC es {0:.0%}'.format(rf_auc) )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e9841c8-c3b8-4465-85d9-8ed0c578715c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0.\n\nAUC is desirable for the following two reasons:\n\nAUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\nAUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b11633e-293d-422d-9972-3371653c9934"}}},{"cell_type":"code","source":["# matriz de confusion \ntp = resultsrf[(resultsrf.Reincidencia == 1) & (resultsrf.prediction== 1)].count()\ntn = resultsrf[(resultsrf.Reincidencia == 0) & (resultsrf.prediction== 0)].count()\nfp = resultsrf[(resultsrf.Reincidencia == 0) & (resultsrf.prediction== 1)].count()\nfn = resultsrf[(resultsrf.Reincidencia == 1) & (resultsrf.prediction== 0)].count()\n\nprint(\"*** Resultados Regresion Logística *** \\n\")\n\nprint(f'True Positive: {tp}')\nprint(f'True Negative: {tn}')\nprint(f'False Positive: {fp}')\nprint(f'False Negative: {fn}')\n\naccuracy=float((tp+tn) /(resultsrf.count()))\nprint(f'Accuracy: {accuracy}')\n\n#recall = float(tp)/(tp + fn)\n#print(f'Recall: {recall}')\n\n#precision = float(tp) / (tp +fp)\n#print(f'Precision: {precision}')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9185470d-cea5-4377-a666-63dfd318b150"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["join_psdf =model_df4.join(rf_predictions,on='features')\ndfFinal = join_psdf.select('IdBeneficiario','prediction')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b12637b-8a2b-4754-87ee-b9bcb50432ce"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Cross validation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc018145-9098-4f23-86fb-d9a7fcfbadb6"}}},{"cell_type":"code","source":["\n\n# Evaluate model\nrfevaluator = BinaryClassificationEvaluator( labelCol='reincidencia')\n\n# Creacion ParamGrid para Cross Validation\n\npipeline = Pipeline(stages=[rf_classifier])\n\nrfparamGrid = (ParamGridBuilder()\n             #.addGrid(rf.maxDepth, [2, 5, 10, 20, 30])\n               .addGrid(rf_classifier.maxDepth, [2, 5, 10, 20, 30])\n             #.addGrid(rf.maxBins, [10, 20, 40, 80, 100])\n               .addGrid(rf_classifier.maxBins, [5, 10, 20, 40, 80, 100])\n             #.addGrid(rf.numTrees, [5, 20, 50, 100, 500])\n               .addGrid(rf_classifier.numTrees, [5, 20, 50, 100, 500])\n             .build())\n\n\n\n#  5-fold CrossValidator\nrfcv = CrossValidator(estimator = pipeline,\n                      estimatorParamMaps = rfparamGrid,\n                      evaluator = rfevaluator,\n                      numFolds = 5)\n\n\n\n# se ejecuta cross validations.\n\n\nrfcvModel = rfcv.fit(train_df)\nprint(rfcvModel)\nrfcvModel\nrfpredictions = rfcvModel.transform(tes_df)\n\n# cvModel uses the best model found from the Cross Validation\n# Evaluacion del modelo\nprint('RMSE:', rfevaluator.evaluate(rfpredictions))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e422bd94-730f-423f-96a1-65d1831d4615"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["> Se guarda el modelo RandomForest"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"30c5b4b7-5c12-4e7a-9955-f3fa9ac70972"}}},{"cell_type":"code","source":["rf_classifier.write().overwrite().save('dbfs:/mnt/gold/icbf-gai/Random_Forest_Classifier/')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70ef6bd1-a245-4ce0-a53e-d8b0be36f0fb"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["> Se hace predicción para toda la base:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0456e10-cf00-4bd5-be34-527c703f831a"}}},{"cell_type":"code","source":["#Transformacion del Clasificador:\nrf_predictionsTotal=rf_classifier.transform(model_df)\n\n#Evaluación del Acuracy:\nrf_accuracyTotal = MulticlassClassificationEvaluator(labelCol='reincidencia',metricName='accuracy').evaluate(rf_predictionsTotal)\n\n#Evaluación de la Precision Ponderada\nrf_precisionTotal = MulticlassClassificationEvaluator(labelCol='reincidencia',metricName='weightedPrecision').evaluate(rf_predictionsTotal)\n\n#Metrics\nprint('El Accuracy Score con los datos de toda la base es {0:.0%}'.format(rf_accuracyTotal))\nprint('El Precision Rate Score con los datos de toda la base es {0:.0%}'.format(rf_precisionTotal))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d2e4a77-77f6-4480-a159-6ea42dab51dc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["rf_predictionsTotal.groupBy('prediction').count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e40d15e-cc38-4efb-8adb-12697239967c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["join_psdf =model_df4.join(rf_predictionsTotal,on='features')\ndfPredicciones = join_psdf.select('IdBeneficiario','prediction')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e9a3f78-6cd0-4c6d-8a59-f5bdbcec3ca0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#---------------------------- Borrar Histórico ----------------------------------------#\ndbfsWriteDelta1 = \"dbfs:/mnt/gold/icbf-gai/BaseModeloProbabilisticoPredicciones/\"\ntry:\n    dbutils.fs.ls(dbfsWriteDelta1)\n    print(\" *** Borrando Directorio *** \")\n    files = dbutils.fs.ls(dbfsWriteDelta1)\n    for f in files:\n        f.path\n        dbutils.fs.rm(f.path, recurse=True)\nexcept:\n    print(\" *** No Existe Directorio *** \")\n    pass\n#---------------------------- Directorio de Salida ------------------------------------#\npathSave = dbfsWriteDelta1\nnombreTabla = \"dbo.dfPredicciones\"\ndfPredicciones.write \\\n.format(\"delta\") \\\n.mode(\"overwrite\") \\\n.option(\"overwriteSchema\", \"true\") \\\n.save(f\"{pathSave}\") \nprint(\" *** Guardado en el DataLake *** \")\nspark.sql(f\"CREATE TABLE IF NOT EXISTS {nombreTabla} USING DELTA LOCATION '{pathSave}/'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2d55983-d294-4ac9-bdbf-fcb14041a976"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Acá se hace el join entre las predicciones y la base descriptiva:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a346b06e-8bf9-4c6f-9aa4-5f40ed862820"}}},{"cell_type":"code","source":["BaseDescriptivaPwBi = spark.read.format(\"delta\").load(\"dbfs:/mnt/gold/icbf-gai/BaseModeloProbabilisticoSabana/\")\nBasePrediccionesPwBi = spark.read.format(\"delta\").load(\"dbfs:/mnt/gold/icbf-gai/BaseModeloProbabilisticoPredicciones/\")\nBaseDescriptivaPwBi.createOrReplaceTempView(\"BaseDescriptivaPwBi\")\nBasePrediccionesPwBi.createOrReplaceTempView(\"BasePrediccionesPwBi\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b202fdf8-eefb-4ad6-bb11-11da4d0a744f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from BaseDescriptivaPwBi"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c04dfcee-5bf2-4269-be5b-d745d4e3441b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["BaseGrafica = spark.sql(\"\"\"\nSELECT\n    BP.*,\n    DB.FechaNacimientoBeneficiario,\n    UPPER(DB.sexoBeneficiario) as sexoBeneficiario,\n    UPPER(COALESCE(DB.NombreSexo,\"No Definido\")) AS NombreSexo,   \n    UPPER(COALESCE(DB.RegimenSeguridadSocial,\"No Definido\")) AS RegimenSeguridadSocial,\n    (YEAR(now()) - YEAR(DB.FechaNacimientoBeneficiario))  AS EdadBeneficiarioAnios, \n    ROUND(DATEDIFF(DATE(now()),DATE(DB.FechaNacimientobeneficiario))/30,0) as EdadBeneficiarioMeses,\n    DB.CodPaisNacimientoBeneficiario,\n    UPPER(COALESCE(DB.NombPaisNacimientoBeneficiario,\"No Definido\")) AS NombPaisNacimientoBeneficiario,   \n    DB.CodPaisRasidenciaBen,\n    UPPER(COALESCE(DB.NombPaisRasidenciaBen,\"No Definido\")) AS NombPaisRasidenciaBen,   \n    CONCAT(DB.NombPaisRasidenciaBen,\", \",DB.NomDepartamentoResidenciaBen,\", \",DB.NomMunicipResidenciaBen,\", Colombia\") AS UbicGeogBen,\n    DB.CodDepartamentoResidenciaBen,\n    UPPER(COALESCE(DB.NomDepartamentoResidenciaBen,\"No Definido\")) AS NomDepartamentoResidenciaBen,   \n    DB.CodMunicipResidenciaBen,\n    UPPER(COALESCE(DB.NomMunicipResidenciaBen,\"No Definido\")) AS NomMunicipResidenciaBen,\n    DB.CodComunaResidenciaBen,\n    UPPER(COALESCE(DB.NomComunaResidenciaBen,\"No Definido\")) AS NomComunaResidenciaBen,\n    DB.CodBarrioResidenciaBen,\n    UPPER(COALESCE(DB.NomBarrioResidenciaBen,\"No Definido\")) AS NomBarrioResidenciaBen,\n    DB.CodVeredaResidenciaBen,\n    UPPER(COALESCE(DB.NomVeredaResidenciaBen,\"No Definido\")) AS NomVeredaResidenciaBen,\n    DB.CodRancheriaResidenciaBen,\n    UPPER(COALESCE(DB.NomRancheriaResidenciaBen,\"No Definido\")) AS NomRancheriaResidenciaBen,\n    UPPER(COALESCE(DB.PresentaDiscapacidad,\"No Definido\")) AS PresentaDiscapacidad,\n    UPPER(COALESCE(DB.ZonaUbicacionBeneficiario,\"No Definido\")) AS ZonaUbicacionBeneficiario,\n    UPPER(COALESCE(DB.GrupoEtnico,\"No Definido\")) AS GrupoEtnico,\n    DB.MesesLactanciaMaternaExclusiva,\n    DB.MesesLactanciaMaternaTotal,\n    DB.PesoBeneficiarioAlNacer,\n    DB.TallaBeneficiarioAlNacer,\n    UPPER(COALESCE(DB.AntecendentePremadurez,\"No Definido\")) AS AntecendentePremadurez,\n    DB.EdadGestacionarAlNacer,\n    left(now(),10) as FechaGeneracion    \nFROM\n    BasePrediccionesPwBi BP\n    INNER JOIN BaseDescriptivaPwBi DB \n\tON BP.IdBeneficiario = DB.IdBeneficiario\n\"\"\")\nBaseGrafica.createOrReplaceTempView(\"BaseGrafica\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"044a1539-e2cf-4c2a-82d7-26ec294244c6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nselect * from BaseGrafica"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b2d0709-71ec-49fa-a0d4-2ed789b9e73c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#---------------------------- Borrar Histórico ----------------------------------------#\ndbfsWriteDelta1 = \"dbfs:/mnt/gold/icbf-gai/BaseModeloProbabilisticoGrafica/\"\ntry:\n    dbutils.fs.ls(dbfsWriteDelta1)\n    print(\" *** Borrando Directorio *** \")\n    files = dbutils.fs.ls(dbfsWriteDelta1)\n    for f in files:\n        f.path\n        dbutils.fs.rm(f.path, recurse=True)\nexcept:\n    print(\" *** No Existe Directorio *** \")\n    pass\n#---------------------------- Directorio de Salida ------------------------------------#\npathSave = dbfsWriteDelta1\nnombreTabla = \"dbo.BaseGrafica\"\nBaseGrafica.write \\\n.format(\"delta\") \\\n.mode(\"overwrite\") \\\n.option(\"overwriteSchema\", \"true\") \\\n.save(f\"{pathSave}\") \nprint(\" *** Guardado en el DataLake *** \")\nspark.sql(f\"CREATE TABLE IF NOT EXISTS {nombreTabla} USING DELTA LOCATION '{pathSave}/'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"220079b3-b2cb-46ed-b5be-85f7d0a41f48"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"03_Modelo","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1126543512521520}},"nbformat":4,"nbformat_minor":0}
